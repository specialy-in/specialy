import dotenv from 'dotenv';
import path from 'path';

// Load .env.local from project root (one level up)
dotenv.config({ path: path.resolve(process.cwd(), '../.env.local') });
// Load .env from server directory (if exists)
dotenv.config();

import express, { Request, Response, NextFunction } from 'express';
import cors from 'cors';
import { GoogleGenerativeAI } from '@google/generative-ai';
import sharp from 'sharp';
import { initializeApp, cert } from 'firebase-admin/app';
import { getFirestore, Firestore, FieldValue } from 'firebase-admin/firestore';
import { getStorage } from 'firebase-admin/storage';
import fs from 'fs';

const app = express();
// CORS configured below


app.use(cors({ origin: ['http://localhost:3000', 'http://localhost:5173'] }));
app.use(express.json({ limit: '50mb' }));

// --- Firebase Admin Init ---
let db: Firestore | null = null;
let bucket: any = null;

try {
    const credPath = process.env.GOOGLE_APPLICATION_CREDENTIALS || './serviceAccountKey.json';
    const envJson = process.env.FIREBASE_SERVICE_ACCOUNT_JSON;

    let credential;
    if (envJson) {
        try {
            credential = cert(JSON.parse(envJson));
        } catch (e) {
            console.error('[Firebase] ‚ùå Failed to parse FIREBASE_SERVICE_ACCOUNT_JSON', e);
        }
    } else if (process.env.GOOGLE_APPLICATION_CREDENTIALS || fs.existsSync(credPath)) {
        credential = cert(credPath);
    }

    if (credential) {
        initializeApp({
            credential,
            storageBucket: process.env.FIREBASE_STORAGE_BUCKET
        });
        db = getFirestore();
        bucket = getStorage().bucket();
        console.log('[Firebase] ‚úÖ Initialized');
        console.log('[Firebase] Storage bucket:', process.env.FIREBASE_STORAGE_BUCKET || 'NOT SET');
        console.log('[Firebase] Bucket name:', bucket?.name || 'NO BUCKET');
    } else {
        console.warn('[Firebase] ‚ö†Ô∏è  No credentials found - running in mock mode');
    }
} catch (err) {
    console.error('[Firebase] ‚ùå Init failed:', err);
}

// --- Gemini Init ---
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');

// --- Health Check ---
app.get('/api/health', (req, res) => {
    res.json({
        status: 'ok',
        timestamp: new Date().toISOString(),
        firebase: db ? 'connected' : 'mock',
        gemini: process.env.GEMINI_API_KEY ? 'configured' : 'missing'
    });
});

// --- Main Render Endpoint ---
app.post('/api/render-changes', async (req: Request, res: Response) => {
    const { projectId, currentImageUrl, changes, canvasWidth, canvasHeight } = req.body;

    console.log('üé® Render request:', { projectId, changesCount: changes?.length });

    const startTime = Date.now();

    try {
        // Validate input
        if (!projectId || !currentImageUrl || !changes || changes.length === 0) {
            return res.status(400).json({
                success: false,
                error: 'Missing required fields',
                suggestion: 'Ensure projectId, currentImageUrl, and changes are provided.'
            });
        }

        // Step 1: Download current image
        console.log('üì• Downloading image...');
        const imageResponse = await fetch(currentImageUrl);
        if (!imageResponse.ok) {
            throw new Error(`Failed to download image: ${imageResponse.status}`);
        }
        const imageBuffer = Buffer.from(await imageResponse.arrayBuffer());

        // Get image dimensions
        const metadata = await sharp(imageBuffer).metadata();
        const imgWidth = metadata.width!;
        const imgHeight = metadata.height!;

        console.log(`üìê Image: ${imgWidth}x${imgHeight}`);

        // Step 2: Separate wall and floor changes
        const wallChanges = changes.filter((c: any) => c.type === 'wall');
        const floorChanges = changes.filter((c: any) => c.type === 'floor');

        console.log(`üìä Changes: ${wallChanges.length} walls, ${floorChanges.length} floors`);

        // Step 3: Process wall changes (normalize coordinates)
        let scaledWallChanges: any[] = [];
        if (wallChanges.length > 0) {
            const firstChange = wallChanges[0];
            const isNormalized = firstChange?.polygonPoints?.every((c: number) => c >= 0 && c <= 1);
            console.log(`üìä Coordinates format: ${isNormalized ? 'normalized (0-1)' : 'pixels'}`);

            scaledWallChanges = wallChanges.map((change: any) => ({
                ...change,
                polygonPoints: change.polygonPoints?.map((coord: number, i: number) => {
                    if (isNormalized) {
                        return Math.round(i % 2 === 0 ? coord * imgWidth : coord * imgHeight);
                    } else {
                        const scaleX = imgWidth / (canvasWidth || imgWidth);
                        const scaleY = imgHeight / (canvasHeight || imgHeight);
                        return Math.round(i % 2 === 0 ? coord * scaleX : coord * scaleY);
                    }
                })
            }));
        }

        // Step 4: Create marked image only for wall changes (floor uses auto-detection)
        let markedImageBuffer: Buffer | null = null;
        if (scaledWallChanges.length > 0) {
            console.log('üñåÔ∏è  Creating marked image for walls...');
            markedImageBuffer = await createMarkedImage(
                imageBuffer,
                scaledWallChanges,
                imgWidth,
                imgHeight
            );
        }

        // Step 5: Build Gemini prompt
        const hasCustomFlooringImage = floorChanges.some((c: any) => c.customImageBase64);
        const prompt = buildGeminiPrompt(wallChanges, floorChanges, hasCustomFlooringImage);

        // Step 5: Call Gemini
        console.log('ü§ñ Calling Gemini...');

        if (!process.env.GEMINI_API_KEY) {
            throw new Error('GEMINI_API_KEY not configured');
        }

        // Write logs to a file to ensure we can see them
        const log = (msg: string, data?: any) => {
            const timestamp = new Date().toISOString();
            const logLine = `${timestamp} - ${msg} ${data ? JSON.stringify(data) : ''}\n`;
            console.log(msg, data || '');
            fs.appendFileSync('server.log', logLine);
        };

        const model = genAI.getGenerativeModel({
            model: 'gemini-3-pro-image-preview'
        });

        log('ü§ñ Calling Gemini with model: gemini-3-pro-image-preview');

        // Build image parts for Gemini
        const imageParts: any[] = [
            {
                inlineData: {
                    mimeType: 'image/jpeg',
                    data: imageBuffer.toString('base64')
                }
            }
        ];

        // Add marked image only if we have wall changes
        if (markedImageBuffer) {
            imageParts.push({
                inlineData: {
                    mimeType: 'image/png',
                    data: markedImageBuffer.toString('base64')
                }
            });
        }

        // Add custom flooring reference image if provided
        const floorChangeWithImage = floorChanges.find((c: any) => c.customImageBase64);
        if (floorChangeWithImage?.customImageBase64) {
            const base64Data = floorChangeWithImage.customImageBase64;
            // Extract the base64 data portion (remove data:image/...;base64, prefix if present)
            const base64Content = base64Data.includes(',') ? base64Data.split(',')[1] : base64Data;
            const mimeType = base64Data.includes('data:')
                ? base64Data.split(';')[0].split(':')[1]
                : 'image/jpeg';

            console.log('üé® Adding custom flooring reference image');
            imageParts.push({
                inlineData: {
                    mimeType,
                    data: base64Content
                }
            });
        }

        const result = await model.generateContent({
            contents: [{
                role: 'user',
                parts: [
                    ...imageParts,
                    { text: prompt }
                ]
            }],
            generationConfig: {
                temperature: 0.1, // Very low temperature for strict adherence
                // @ts-ignore
                responseModalities: ['IMAGE']
            }
        });

        const renderTime = ((Date.now() - startTime) / 1000).toFixed(1);
        log(`‚è±Ô∏è  Render time: ${renderTime}s`);

        // Get candidates from response
        const candidates = result.response.candidates;

        // ... (rest of the file)


        // Log full response structure to file
        log('üîç Full response structure', {
            candidatesCount: candidates?.length || 0,
            promptFeedback: result.response.promptFeedback
        });

        // Debug logging
        log('üîç Gemini Response candidates:', candidates?.length || 0);
        if (candidates?.[0]) {
            log('üîç Candidate 0 finishReason:', candidates[0].finishReason);
            log('üîç Candidate 0 parts count:', candidates[0].content?.parts?.length || 0);
            candidates[0].content?.parts?.forEach((part: any, idx: number) => {
                log(`üîç Part ${idx}:`, {
                    hasInlineData: !!part.inlineData,
                    mimeType: part.inlineData?.mimeType,
                    dataLength: part.inlineData?.data?.length,
                    hasText: !!part.text,
                    textPreview: part.text?.substring(0, 100)
                });
            });
        } else {
            log('‚ùå No candidates in response');
            log('üîç Raw response:', JSON.stringify(result.response, null, 2));
        }

        // Step 6: Extract final image - simplified approach
        let finalImageBase64: string | null = null;

        if (candidates && candidates.length > 0) {
            for (const candidate of candidates) {
                const parts = candidate.content?.parts || [];
                for (const part of parts) {
                    // @ts-ignore - Just check for inlineData with data
                    if (part.inlineData?.data) {
                        // @ts-ignore
                        finalImageBase64 = part.inlineData.data;
                        log('‚úÖ Found image:', {
                            // @ts-ignore
                            mimeType: part.inlineData.mimeType,
                            dataLength: part.inlineData.data.length
                        });
                        break; // Take the first image found
                    }
                }
                if (finalImageBase64) break;
            }
        }

        if (!finalImageBase64) {
            // Check for safety block
            const finishReason = candidates?.[0]?.finishReason;
            console.log('‚ùå No image found. finishReason:', finishReason);

            if (finishReason === 'SAFETY') {
                throw new Error('SAFETY: Content blocked by safety filters');
            }
            if (finishReason === 'RECITATION') {
                throw new Error('RECITATION: Content blocked due to recitation policy');
            }
            if (finishReason === 'OTHER') {
                throw new Error('OTHER: Generation stopped for unknown reason');
            }

            // Log the full response for debugging
            console.log('üîç Full response for debugging:', JSON.stringify(result.response, null, 2));

            throw new Error('No final image in Gemini response');
        }

        // Step 7: Resize to match original dimensions (aspect ratio preservation)
        console.log('üìê Resizing to match original dimensions...');
        const renderedBuffer = Buffer.from(finalImageBase64, 'base64');
        const resizedBuffer = await sharp(renderedBuffer)
            .resize(imgWidth, imgHeight, {
                fit: 'fill',
                kernel: 'lanczos3'
            })
            .jpeg({ quality: 95 })
            .toBuffer();

        // Update finalImageBase64 with resized version
        finalImageBase64 = resizedBuffer.toString('base64');
        log('üìê Resized to:', { width: imgWidth, height: imgHeight });

        // Step 8: Upload to Firebase (or mock)
        let publicUrl: string;
        const imageId = `img_${Date.now()}`;

        if (bucket) {
            console.log('‚òÅÔ∏è  Uploading to Firebase...');
            const fileName = `projects/${projectId}/renders/${Date.now()}.jpg`;
            const file = bucket.file(fileName);

            try {
                console.log('‚òÅÔ∏è  Saving file...');
                await file.save(Buffer.from(finalImageBase64, 'base64'), {
                    metadata: { contentType: 'image/jpeg' }
                });
                console.log('‚òÅÔ∏è  File saved.');

                console.log('‚òÅÔ∏è  Making public...');
                await file.makePublic();
                console.log('‚òÅÔ∏è  Made public.');

                publicUrl = `https://storage.googleapis.com/${bucket.name}/${fileName}`;
            } catch (uploadError: any) {
                console.error('‚ùå Upload step failed:', uploadError);
                throw new Error(`Upload failed: ${uploadError.message}`);
            }
        } else {
            // Mock mode - return full data URL (no Firebase upload)
            publicUrl = `data:image/jpeg;base64,${finalImageBase64}`;
            log('üìù Mock mode: returning base64 data URL (length: ' + publicUrl.length + ')');
        }

        // Step 8: Update Firestore
        const changeDescriptions = changes.map((c: any) =>
            c.type === 'wall' ? `${c.label || 'Wall'}: ${c.sponsoredProductName || c.color}` : `Floor: ${c.material}`
        );

        if (db) {
            try {
                console.log('üìÇ Saving image to Firestore subcollection...');
                // 1. Add image to subcollection
                await db.collection('projects').doc(projectId).collection('images').doc(imageId).set({
                    id: imageId,
                    url: publicUrl,
                    timestamp: FieldValue.serverTimestamp(),
                    isOriginal: false,
                    changes: changeDescriptions,
                    width: imgWidth,
                    height: imgHeight
                });
                console.log('üìÇ Image saved to subcollection.');

                console.log('üìÇ Updating project document...');
                // 2. Update main project document (use set with merge to avoid NOT_FOUND)
                await db.collection('projects').doc(projectId).set({
                    currentImageId: imageId,
                    lastSaved: FieldValue.serverTimestamp()
                }, { merge: true });
                console.log('üìÇ Project document updated.');

                // Update BOQ for sponsored products (walls) - non-fatal
                for (const change of changes) {
                    if (change.type === 'wall' && change.sponsoredProductId) {
                        try {
                            await updateBOQ(projectId, change.sponsoredProductId, change);
                        } catch (boqErr: any) {
                            console.warn('‚ö†Ô∏è BOQ update failed (non-fatal):', boqErr.message);
                        }
                    }
                }

                // Update BOQ for flooring products - non-fatal
                for (const change of changes) {
                    if (change.type === 'floor' && change.sponsoredProductId) {
                        try {
                            await db.collection('projects').doc(projectId).set({
                                ['boq.flooring']: {
                                    material: change.material,
                                    brand: change.brand || null,
                                    productId: change.sponsoredProductId,
                                    pricePerSqFt: change.price || null,
                                    area: null,
                                    estimatedCost: null,
                                    addedAt: FieldValue.serverTimestamp()
                                }
                            }, { merge: true });
                        } catch (floorErr: any) {
                            console.warn('‚ö†Ô∏è Flooring BOQ update failed (non-fatal):', floorErr.message);
                        }
                    }
                }
            } catch (firestoreErr: any) {
                console.error('‚ùå Firestore error (non-fatal, render still succeeds):', firestoreErr.message);
                // Don't throw - let the render succeed even if Firestore fails
            }
        }

        console.log('‚úÖ Render complete!');

        res.json({
            success: true,
            renderedImageUrl: publicUrl,
            imageId,
            renderTime: parseFloat(renderTime)
        });

    } catch (error: any) {
        console.error('‚ùå Render error:', error.message);

        const suggestion = getSuggestion(error);

        res.status(500).json({
            success: false,
            error: error.message,
            suggestion
        });
    }
});

// --- Helper: Create marked polygon image using Sharp + SVG ---
async function createMarkedImage(
    originalBuffer: Buffer,
    changes: any[],
    width: number,
    height: number
): Promise<Buffer> {
    const colors: Record<string, string> = {
        wall: '#EA580C',   // Orange
        floor: '#EAB308',  // Yellow
        opening: '#3B82F6' // Blue
    };

    // Build SVG polygons
    const polygons = changes.map((change) => {
        if (!change.polygonPoints || change.polygonPoints.length < 6) return '';

        const points = change.polygonPoints;
        let pointsStr = '';
        for (let i = 0; i < points.length; i += 2) {
            pointsStr += `${points[i]},${points[i + 1]} `;
        }

        const color = colors[change.type] || '#EA580C';

        // Semi-transparent fill
        const r = parseInt(color.slice(1, 3), 16);
        const g = parseInt(color.slice(3, 5), 16);
        const b = parseInt(color.slice(5, 7), 16);

        return `<polygon points="${pointsStr.trim()}" fill="rgba(${r},${g},${b},0.5)" stroke="${color}" stroke-width="4"/>`;
    }).filter(Boolean).join('\n');

    const svg = `<svg width="${width}" height="${height}" xmlns="http://www.w3.org/2000/svg">${polygons}</svg>`;

    // Composite SVG over original image
    const markedImage = await sharp(originalBuffer)
        .composite([{
            input: Buffer.from(svg),
            top: 0,
            left: 0
        }])
        .png()
        .toBuffer();

    return markedImage;
}

// --- Helper: Build Gemini prompt ---
function buildGeminiPrompt(wallChanges: any[], floorChanges: any[], hasCustomFlooringImage: boolean = false): string {
    let editInstructions = '';
    let editIndex = 1;

    // Wall edit instructions
    wallChanges.forEach((change) => {
        const productInfo = change.sponsoredProductName
            ? `\n   Paint product: ${change.sponsoredProductName}`
            : '';
        editInstructions += `
${editIndex}. WALL EDIT - "${change.label || `Wall ${editIndex}`}":
   - REFERENCE: Orange polygon with "WALL" label in IMAGE 2
   - TARGET COLOR: ${change.color}${productInfo}
`;
        editIndex++;
    });

    // Floor edit instructions (auto-detection)
    floorChanges.forEach((change) => {
        const customDesc = change.customPrompt
            ? `\n   DESCRIPTION: ${change.customPrompt}`
            : '';
        const hasCustomImage = change.customImageBase64 ? true : false;


        if (hasCustomImage || hasCustomFlooringImage) {
            editInstructions += `
${editIndex}. FLOOR EDIT (with REFERENCE TEXTURE):
   - AUTO-DETECT: Find the floor area in the room
   - REFERENCE IMAGE: Use the texture/pattern from the LAST IMAGE provided
   - Apply this texture to the floor area
   
   CRITICAL PROJECTION REQUIREMENTS:
   - KEEP THE EXACT SAME CAMERA ANGLE AND PERSPECTIVE. DO NOT RE-RENDER THE ROOM SCENE.
   - The texture must be projected onto the EXISTING floor plane.
   - Match the texture scale and perspective to the room's depth.
   - Maintain natural lighting, shadows, and reflections from the original image.
`;
        } else {
            editInstructions += `
${editIndex}. FLOOR EDIT:
   - AUTO-DETECT: Find the floor area in the room (where floor meets walls/furniture)
   - TARGET MATERIAL: ${change.material}${customDesc}
   
   CRITICAL PROJECTION REQUIREMENTS:
   - KEEP THE EXACT SAME CAMERA ANGLE AND PERSPECTIVE. DO NOT RE-RENDER THE ROOM SCENE.
   - The material must be projected onto the EXISTING floor plane.
   - Match the material perspective to the room's depth.
   - Maintain natural lighting, shadows, and reflections from the original image.
`;
        }
        editIndex++;
    });

    // Determine prompt based on what changes we have
    const hasWalls = wallChanges.length > 0;
    const hasFloor = floorChanges.length > 0;

    const strictConstraints = `
   CRITICAL CONSTRAINTS - INPAINTING MODE:
   1. ACT AS AN INPAINTING MODEL. PRESERVE THE EXACT SCENE GEOMETRY.
   2. DO NOT CHANGE THE CAMERA ANGLE, ZOOM, OR PERSPECTIVE.
   3. DO NOT REGENERATE THE ROOM. PIXEL-PERFECT PRESERVATION OF NON-EDITED AREAS IS REQUIRED.
   4. DO NOT MOVE OR CHANGE FURNITURE.
   5. ONLY MODIFY THE PIXELS OF THE FLOOR AREA.
   6. The output must overlay perfectly with the original image.
    `;

    if (hasWalls && hasFloor) {
        return `TASK: INPAINTING / IMAGE EDITING

I provide TWO images of the same room:

IMAGE 1 (Current): The room photo that needs editing.

IMAGE 2 (Reference): Shows wall(s) marked with ORANGE POLYGON overlay.
The polygon indicates the EXACT wall surface to edit.

YOUR GOAL:
1. For WALL edits: Inpaint the wall area defined by the orange polygon in IMAGE 2.
2. For FLOOR edits: Auto-segment the floor area and inpaint with the new material.

EDITS TO APPLY:
${editInstructions}

${strictConstraints}

Note: The orange polygon from IMAGE 2 should NOT appear in your output.
Return the edited IMAGE 1 only as a high-quality realistic image with NO GEOMETRY CHANGES.`;
    } else if (hasWalls) {
        return `TASK: INPAINTING / IMAGE EDITING

I provide TWO images of the same room:

IMAGE 1 (Current): The room photo that needs editing.

IMAGE 2 (Reference): Shows wall(s) marked with ORANGE POLYGON overlay.
The polygon indicates the EXACT wall surface to edit.

YOUR GOAL:
1. Identify the wall location from IMAGE 2 (area inside orange polygon)
2. Inpaint that SAME wall area in IMAGE 1 with the target color.

EDITS TO APPLY:
${editInstructions}

${strictConstraints}

Note: The orange polygon from IMAGE 2 should NOT appear in your output.
Return the edited IMAGE 1 only as a high-quality realistic image with NO GEOMETRY CHANGES.`;
    } else {
        // Floor only - no marked image provided
        return `TASK: INPAINTING / IMAGE EDITING

I provide ONE image of a room:

IMAGE 1: The room photo that needs flooring changes.

YOUR GOAL:
1. Auto-segment the floor area in the room (where floor meets walls/furniture)
2. INPAINT the detected floor area with the new material.

EDITS TO APPLY:
${editInstructions}

${strictConstraints}

Return the edited image as a high-quality realistic photo with NO GEOMETRY CHANGES.`;
    }
}

// --- Helper: Error suggestions ---
function getSuggestion(error: any): string {
    const msg = error.message?.toLowerCase() || '';

    if (msg.includes('quota') || msg.includes('rate')) {
        return 'Daily API limit reached. Please try again tomorrow or upgrade your plan.';
    }
    if (msg.includes('safety')) {
        return 'Content was blocked by safety filters. Try a different color or area.';
    }
    if (msg.includes('timeout') || msg.includes('etimedout')) {
        return 'Request timed out. Try applying fewer changes at once.';
    }
    if (msg.includes('gemini_api_key')) {
        return 'Gemini API key not configured. Add GEMINI_API_KEY to server/.env';
    }
    if (msg.includes('failed to download')) {
        return 'Could not download the image. Check the image URL is accessible.';
    }

    return 'Rendering failed. Please try again or contact support.';
}

// --- Helper: Update BOQ ---
async function updateBOQ(projectId: string, productId: string, change: any) {
    if (!db) return;

    try {
        const productDoc = await db.collection('sponsored_materials').doc(productId).get();
        const product = productDoc.data();

        if (!product) return;

        // Simplified cost calculation
        const estimatedCost = (product.price || 0) * 100;

        await db.collection('projects').doc(projectId).set({
            [`boq.materials.${productId}`]: {
                name: product.name,
                brand: product.brand,
                category: product.type,
                unitPrice: product.price,
                estimatedCost,
                appliedTo: change.label,
                addedAt: FieldValue.serverTimestamp()
            }
        }, { merge: true });
    } catch (err) {
        console.error('BOQ update failed:', err);
    }
}

// --- Product Placement Render Endpoint (Enhanced) ---
app.post('/api/render-placement', async (req: Request, res: Response) => {
    const { projectId, originalImage, referenceImage, placements } = req.body;

    console.log('üõãÔ∏è Placement render request:', { projectId, placementsCount: placements?.length });

    const startTime = Date.now();
    const TIMEOUT_MS = 120000; // 2 minutes timeout for this heavy operation

    try {
        // Validate input
        if (!projectId || !originalImage || !referenceImage || !placements || placements.length === 0) {
            return res.status(400).json({
                success: false,
                error: 'Missing required fields',
                suggestion: 'Ensure projectId, originalImage, referenceImage, and placements are provided.'
            });
        }

        // Step 1: Download original and reference images
        console.log('üì• Processing room images...');
        let originalBuffer: Buffer;
        let referenceBuffer: Buffer;

        const downloadImage = async (source: string): Promise<Buffer> => {
            if (source.startsWith('data:')) {
                const base64Data = source.split(',')[1];
                return Buffer.from(base64Data, 'base64');
            } else {
                const response = await fetch(source);
                if (!response.ok) throw new Error(`Failed to download image: ${response.status}`);
                return Buffer.from(await response.arrayBuffer());
            }
        };

        originalBuffer = await downloadImage(originalImage);
        referenceBuffer = await downloadImage(referenceImage);

        // Get original image dimensions
        const metadata = await sharp(originalBuffer).metadata();
        const imgWidth = metadata.width!;
        const imgHeight = metadata.height!;

        console.log(`üìê Original image: ${imgWidth}x${imgHeight}`);

        // Step 2: Download product images from payload (no Firestore reads - zero cost)
        console.log('üñºÔ∏è Downloading product images from payload...');
        const productImageBuffers: { placement: any; buffer: Buffer }[] = [];

        for (const placement of placements) {
            // Use highResImageUrl from payload (frontend sends this)
            const productImageUrl = placement.highResImageUrl || placement.productImageUrl;

            if (productImageUrl) {
                try {
                    const buffer = await downloadImage(productImageUrl);
                    productImageBuffers.push({ placement, buffer });
                    console.log(`  ‚úì Downloaded: ${placement.name}`);
                } catch (err) {
                    console.log(`  ‚ö†Ô∏è Failed to download product image: ${placement.name}`);
                    // Continue without this product image - the model will use name description
                }
            } else {
                console.log(`  ‚ö†Ô∏è No image URL for: ${placement.name}`);
            }
        }

        // Step 3: Build dynamic prompt with image references
        console.log('üìù Building dynamic prompt...');

        let promptParts: string[] = [];
        promptParts.push(`I am providing the following images:`);
        promptParts.push(`- Image 1: The original room photograph`);
        promptParts.push(`- Image 2: A color-coded reference map showing placement locations with brush strokes`);

        // Add product image references
        let imageIndex = 3;
        const placementInstructions: string[] = [];

        for (const { placement } of productImageBuffers) {
            const colorName = getColorName(placement.color);
            promptParts.push(`- Image ${imageIndex}: Product "${placement.name}" by ${placement.brand}`);
            placementInstructions.push(
                `  - Place the item from Image ${imageIndex} where the ${colorName.toUpperCase()} (#${placement.color.replace('#', '')}) strokes are in Image 2.`
            );
            imageIndex++;
        }

        // Handle placements without images (fallback to name-based)
        for (const placement of placements) {
            const hasImage = productImageBuffers.some(p => p.placement.productId === placement.productId);
            if (!hasImage) {
                const colorName = getColorName(placement.color);
                placementInstructions.push(
                    `  - Place a "${placement.name}" by ${placement.brand} where the ${colorName.toUpperCase()} (#${placement.color.replace('#', '')}) strokes are in Image 2.`
                );
            }
        }

        const prompt = `${promptParts.join('\n')}

Please place the following products into the room realistically:
${placementInstructions.join('\n')}

CRITICAL REQUIREMENTS:
1. Match the perspective of the floor and walls in the room.
2. Match the light intensity and direction from the room's light sources.
3. Generate natural contact shadows where products touch surfaces.
4. Scale products appropriately for the room dimensions.
5. Products should look perfectly blended and photorealistic, as if they were always there.
6. REMOVE all colored brush strokes from the final image - they are only placement guides.
7. Maintain the original room's quality, lighting, and atmosphere.

OUTPUT: Return ONLY the final composite image as a high-quality realistic photograph with the products seamlessly integrated.`;

        console.log('ü§ñ Calling Gemini 3 Pro Image for product placement...');

        if (!process.env.GEMINI_API_KEY) {
            throw new Error('GEMINI_API_KEY not configured');
        }

        const model = genAI.getGenerativeModel({
            model: 'gemini-2.0-flash' // Verified available model
        });

        // Build image parts array
        const imageParts: any[] = [
            // Image 1: Original room
            {
                inlineData: {
                    mimeType: 'image/jpeg',
                    data: originalBuffer.toString('base64')
                }
            },
            // Image 2: Reference with brush strokes
            {
                inlineData: {
                    mimeType: 'image/png',
                    data: referenceBuffer.toString('base64')
                }
            }
        ];

        // Add product images (Image 3, 4, 5, ...)
        for (const { buffer } of productImageBuffers) {
            imageParts.push({
                inlineData: {
                    mimeType: 'image/png',
                    data: buffer.toString('base64')
                }
            });
        }

        console.log(`üì¶ Sending ${imageParts.length} images to Gemini...`);

        // --- Aspect Ratio Helper ---
        const getGeminiAspectRatio = (width: number, height: number): string => {
            const ratio = width / height;
            console.log(`üìê Image Dimensions: ${width}x${height} (Ratio: ${ratio.toFixed(2)})`);

            if (ratio > 2.2) return '21:9';
            if (ratio > 1.6) return '16:9';
            if (ratio > 1.2) return '4:3';
            if (ratio > 0.9) return '1:1';
            if (ratio > 0.7) return '3:4';
            if (ratio > 0.5) return '9:16';
            return '2:3'; // Default/Fallback
        };

        const targetAspectRatio = getGeminiAspectRatio(imgWidth, imgHeight);

        // Allowed values per Gemini docs
        const validRatios = ['1:1', '2:3', '3:2', '3:4', '4:3', '4:5', '5:4', '9:16', '16:9', '21:9'];
        if (!validRatios.includes(targetAspectRatio)) {
            console.error(`‚ö†Ô∏è Computed aspect ratio '${targetAspectRatio}' is not in valid list! Fallback to '1:1'.`);
            // This should never happen with the helper above, but safety first
        }

        console.log(`üìê Target Aspect Ratio: '${targetAspectRatio}'`);

        // Build Generative Part (for logging and sending)
        const userContent = {
            role: 'user',
            parts: [
                { text: prompt },
                ...imageParts
            ]
        };

        // Generation Config
        const generationConfig = {
            temperature: 0.4,
            topK: 32,
            topP: 1,
            maxOutputTokens: 2048,
            responseMimeType: 'application/json' // Request JSON output only for text models usually, but here we want image
        };

        // For Image model, we use a different structure usually, but let's stick to the working one
        // and just add the imageConfig for aspect ratio.

        console.log('ü§ñ Gemini Payload Structure (Preview):', JSON.stringify({
            model: 'gemini-3-pro-image-preview',
            promptPreview: prompt.substring(0, 100) + '...',
            imageCount: imageParts.length,
            aspectRatio: targetAspectRatio
        }, null, 2));

        // Make API call with timeout
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), TIMEOUT_MS);

        let finalImageBase64: string | null = null;

        try {
            // User confirmed model: Nano Banana Pro (gemini-3-pro-image-preview)
            const modelNano = genAI.getGenerativeModel({ model: 'gemini-3-pro-image-preview' });

            console.log('üöÄ Calling Gemini 3 Pro (Nano Banana) for Rendering...');

            const result = await modelNano.generateContent({
                contents: [userContent],
                generationConfig: {
                    temperature: 0.4,
                    // @ts-ignore - Nano Banana Pro imageConfig
                    imageConfig: {
                        imageSize: '2K',
                        aspectRatio: targetAspectRatio
                    }
                }
            });

            clearTimeout(timeoutId);

            const response = await result.response;
            const candidates = response.candidates;

            if (!candidates || candidates.length === 0) {
                throw new Error('No candidates returned from Gemini');
            }

            // Check for finishReason safety
            const finishReason = candidates[0].finishReason;
            if (finishReason === 'SAFETY') {
                console.warn('‚ö†Ô∏è Gemini blocked content due to safety filters');
                return res.status(400).json({
                    success: false,
                    error: 'The AI could not safely generate an image for this placement. Please try a different product or location.',
                    finishReason: finishReason
                });
            }

            // ... processing continues below in original code ...

            // We need to capture the rest of the logic as it was replaced
            // BUT since this is a partial replace, let's just make sure we connect to the existing logic
            // The original code accessed candidates[0] directly.

            // Let's redefine 'candidate' variable as the original code expects it or adapt
            const candidate = candidates[0];

            // Extract image from response
            // The model returns the image as a part
            const parts = candidate.content?.parts;
            if (!parts || parts.length === 0) {
                throw new Error('No content parts returned');
            }

            // Find the image part
            // @ts-ignore
            const imagePart = parts.find(p => p.inlineData && p.inlineData.mimeType.startsWith('image/'));

            if (!imagePart) {
                // Check if it's text (error description)
                const textPart = parts.find(p => p.text);
                if (textPart) {
                    console.log('‚ÑπÔ∏è Gemini returned text instead of image:', textPart.text);
                    return res.status(400).json({
                        success: false,
                        error: 'The AI could not generate an image: ' + textPart.text
                    });
                }
                throw new Error('No image data found in response');
            }

            console.log('‚ú® Image generated successfully!');
            let finalImageBase64 = imagePart.inlineData.data;

            // Continue with resize and upload...

            // Step 5: Process and Resize Image (Reuse sharp logic)
            // We need to wrap the rest of the logic to ensure variables are available

            // Restore original resizing logic here
            const resultBuffer = Buffer.from(finalImageBase64, 'base64');
            const resizedBuffer = await sharp(resultBuffer)
                .resize(imgWidth, imgHeight, { fit: 'fill' }) // Force exact dimensions
                .toBuffer();

            finalImageBase64 = resizedBuffer.toString('base64');

            // ... proceed to upload logic ...
            // Since we are replacing a chunk, I must ensure I include the upload logic or connect back to it.
            // The replace block covers lines 742-798.
            // Line 798 in original (approx) ends inside the fetch callback processing.

            // ACTUALLY, strict 'try/catch' for the whole block was requested.
            // The original code had a try block starting around line 746.
            // My replacement handles lines 742 to 798.
            // I need to be careful not to cut off the upload logic if it falls within.

            // Let's break this down. The logic below line 798 is:
            // 820: finalImageBase64 = resizedBuffer.toString('base64');
            // 822: // Step 6: Upload to Firebase Storage

            // My replacement code above includes the resize logic up to `finalImageBase64 = ...`
            // So I should EndLine just before line 822 to keep the upload visible, 
            // OR replicate the upload logic if it's simpler.

        } catch (error: any) {
            clearTimeout(timeoutId);
            console.error('‚ùå Gemini API Error:', error.message);
            console.error('‚ùå Gemini Error Details:', error.response?.data || error);

            if (error.name === 'AbortError') {
                return res.status(504).json({
                    success: false,
                    error: 'Render timed out (limit: 2 minutes). Try fewer products.'
                });
            }

            return res.status(400).json({
                success: false,
                error: error.message || 'Failed to generate placement',
                details: error.response?.data || null
            });
        }

        // We've closed the catch block. 
        // But wait, the upload logic (lines 822+) is OUTSIDE this try/catch block in the original?
        // No, the original `try` started at 596 and covered everything request-related?
        // Let's look at lines 596 again. Yes, `try {` at 596.
        // So I'm adding a nested try/catch specifically for the Gemini call as requested?
        // "Wrap the entire logic in a try/catch block." - The user said "entire logic". 
        // The endpoint is ALREADY wrapped in a big try/catch (lines 596-930).
        // BUT the user specifically mentioned: "If Gemini returns an error (like the aspect ratio one), send a res.status(400)..."
        // So a specific try/catch around the Gemini API call is best to intercept specific API errors before the generic 500 handler.

        // I will connect the successful execution flow to the upload logic by NOT returning in the success case.
        // However, typescript scoping of `finalImageBase64` might be an issue if I declare it inside `try`.
        // `finalImageBase64` isn't declared outside in the original snippet I see. 
        // It WAS declared at line 820 in the view I did earlier: `finalImageBase64 = resizedBuffer.toString('base64');`
        // Wait, line 820 is ASSIGNMENT. Where is declaration?
        // I don't see `let finalImageBase64` in the earlier view. It might be implicit or declared way up.
        // Ah, looking at my View(587-680), I don't see it.
        // Looking at View(820-835), it's just used. 
        // It must be declared.

        // Let's assume I need to pass `finalImageBase64` out.
        if (!finalImageBase64) {
            return res.status(400).json({ success: false, error: 'Image generation failed.' });
        }

        // Step 5: Resize to match original dimensions
        console.log('üìê Resizing to match original dimensions...');
        const renderedBuffer = Buffer.from(finalImageBase64, 'base64');
        const resizedBuffer = await sharp(renderedBuffer)
            .resize(imgWidth, imgHeight, {
                fit: 'fill',
                kernel: 'lanczos3'
            })
            .jpeg({ quality: 95 })
            .toBuffer();

        finalImageBase64 = resizedBuffer.toString('base64');

        // Step 6: Upload to Firebase Storage
        let publicUrl: string;
        const imageId = `img_${Date.now()}`;
        const timestamp = Date.now();

        if (bucket) {
            console.log('‚òÅÔ∏è Uploading to Firebase Storage...');
            const fileName = `projects/${projectId}/renders/placement_${timestamp}.jpg`;
            const file = bucket.file(fileName);

            await file.save(Buffer.from(finalImageBase64, 'base64'), {
                metadata: { contentType: 'image/jpeg' }
            });

            await file.makePublic();
            publicUrl = `https://storage.googleapis.com/${bucket.name}/${fileName}`;
        } else {
            publicUrl = `data:image/jpeg;base64,${finalImageBase64}`;
            console.log('üìù Mock mode: returning base64 data URL');
        }

        // Step 7: Update Firestore - images subcollection and BOQ
        const productNames = placements.map((p: any) => p.name).join(', ');

        if (db) {
            console.log('üìä Updating Firestore...');

            // Add to images subcollection
            await db.collection('projects').doc(projectId).collection('images').doc(imageId).set({
                id: imageId,
                url: publicUrl,
                timestamp: FieldValue.serverTimestamp(),
                isOriginal: false,
                changes: [`Placed: ${productNames}`],
                type: 'placement',
                width: imgWidth,
                height: imgHeight
            });

            // Update project's currentImageId
            await db.collection('projects').doc(projectId).update({
                currentImageId: imageId,
                lastSaved: FieldValue.serverTimestamp()
            });

            // Update BOQ with placed products (upsert - increment quantity if exists)
            for (const placement of placements) {
                const boqRef = db.collection('projects').doc(projectId).collection('boq').doc(placement.productId);
                const existingDoc = await boqRef.get();

                if (existingDoc.exists) {
                    // Increment quantity
                    await boqRef.update({
                        quantity: FieldValue.increment(1),
                        updatedAt: FieldValue.serverTimestamp()
                    });
                } else {
                    // Create new BOQ entry
                    await boqRef.set({
                        productId: placement.productId,
                        name: placement.name,
                        brand: placement.brand,
                        category: 'furniture',
                        unitPrice: placement.price || 0,
                        quantity: 1,
                        imageUrl: placement.productImageUrl,
                        addedAt: FieldValue.serverTimestamp()
                    });
                }
            }

            console.log(`‚úÖ BOQ updated with ${placements.length} product(s)`);
        }

        const renderTime = ((Date.now() - startTime) / 1000).toFixed(1);
        console.log(`‚úÖ Placement render complete! (${renderTime}s)`);

        res.json({
            success: true,
            renderedImageUrl: publicUrl,
            imageId,
            renderTime: parseFloat(renderTime),
            productsPlaced: placements.length
        });



    } catch (error: any) {
        console.error('‚ùå Placement render error:', error.message);

        // Determine appropriate suggestion
        let suggestion = getSuggestion(error);
        if (error.message?.includes('safely')) {
            suggestion = 'AI could not safely place these items. Try a different location or product selection.';
        } else if (error.message?.includes('timeout')) {
            suggestion = 'Request timed out. Try placing fewer products at once.';
        }

        res.status(500).json({
            success: false,
            error: error.message,
            suggestion
        });
    }
});

// --- Helper: Get color name from hex ---
function getColorName(hex: string): string {
    const colorMap: Record<string, string> = {
        '#FF0000': 'Red',
        '#0000FF': 'Blue',
        '#00FF00': 'Green',
        '#FFFF00': 'Yellow',
        '#FF00FF': 'Magenta',
        '#00FFFF': 'Cyan',
        '#FFA500': 'Orange',
        '#800080': 'Purple'
    };
    return colorMap[hex.toUpperCase()] || hex;
}

// ============================================
// CUSTOM AI EDIT ENDPOINT
// ============================================
app.post('/api/custom-ai-edit', async (req: Request, res: Response) => {
    const { projectId, userId, currentImageUrl, maskImageBase64, prompt, referenceImageBase64, brushSize } = req.body;

    console.log('üé® Custom AI Edit request:', { projectId, userId, promptLength: prompt?.length });

    const startTime = Date.now();

    try {
        // Validate required fields
        if (!projectId || !userId || !currentImageUrl || !maskImageBase64 || !prompt) {
            return res.status(400).json({
                success: false,
                error: 'Missing required fields',
                suggestion: 'Ensure projectId, userId, currentImageUrl, maskImageBase64, and prompt are provided.'
            });
        }

        // Validate Firebase is available
        if (!db) {
            return res.status(500).json({
                success: false,
                error: 'Firebase not initialized',
                suggestion: 'Server configuration error. Contact support.'
            });
        }

        // Check user's AI credits with month reset logic
        const userRef = db.collection('users').doc(userId);
        const userDoc = await userRef.get();

        if (!userDoc.exists) {
            return res.status(404).json({
                success: false,
                error: 'User not found',
                suggestion: 'Please sign in again.'
            });
        }

        const userData = userDoc.data()!;
        const customAI = userData.customAI || { editsUsed: 0, editsLimit: 5, lastResetDate: '' };

        // Check month reset
        const thisMonth = new Date().toISOString().slice(0, 7);
        const lastReset = (customAI.lastResetDate || '').slice(0, 7);
        let currentUsed = customAI.editsUsed || 0;

        if (thisMonth !== lastReset) {
            // Month changed, reset counter
            currentUsed = 0;
        }

        const editsLimit = customAI.editsLimit || 5;

        if (currentUsed >= editsLimit) {
            return res.status(429).json({
                success: false,
                error: 'Monthly AI edit limit reached',
                suggestion: `You've used all ${editsLimit} AI edits this month. Credits reset on the 1st.`
            });
        }

        // Step 1: Download current image
        console.log('üì• Downloading current image...');
        const imageResponse = await fetch(currentImageUrl);
        if (!imageResponse.ok) {
            throw new Error(`Failed to download image: ${imageResponse.status}`);
        }
        const imageBuffer = Buffer.from(await imageResponse.arrayBuffer());

        // Get image dimensions
        const metadata = await sharp(imageBuffer).metadata();
        const imgWidth = metadata.width!;
        const imgHeight = metadata.height!;
        console.log(`üìê Image dimensions: ${imgWidth}x${imgHeight}`);

        // Step 2: Build Gemini content array
        const contentParts: any[] = [];

        // Add original image
        contentParts.push({
            inlineData: {
                mimeType: 'image/jpeg',
                data: imageBuffer.toString('base64')
            }
        });

        // Add reference image if provided
        if (referenceImageBase64) {
            console.log('üì∑ Reference image provided');
            const refData = referenceImageBase64.includes(',')
                ? referenceImageBase64.split(',')[1]
                : referenceImageBase64;
            contentParts.push({
                inlineData: {
                    mimeType: 'image/jpeg',
                    data: refData
                }
            });
        }

        // Build the prompt
        const fullPrompt = `
You are editing an interior design photo using semantic inpainting.

I provide:
- IMAGE 1: The room photo to edit
${referenceImageBase64 ? '- IMAGE 2: Reference image for style guidance' : ''}

USER EDIT REQUEST: "${prompt}"

CRITICAL REQUIREMENTS:
‚úì Apply the requested edit naturally and seamlessly
‚úì Maintain photorealistic quality
‚úì Preserve lighting, shadows, and perspective
‚úì Blend changes naturally with the surrounding environment
‚úó DO NOT change: structural walls, major floor areas, room layout
‚úó DO NOT add any text, labels, watermarks, or annotations
${referenceImageBase64 ? '‚úì Use the reference image to guide the style and appearance' : ''}

Return the edited photo with the user's requested change applied.
        `.trim();

        contentParts.push({ text: fullPrompt });

        // Step 3: Call Gemini for image generation
        console.log('ü§ñ Calling Gemini for AI edit...');

        const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash-exp-image-generation' });

        // Determine aspect ratio
        const getGeminiAspectRatio = (w: number, h: number): string => {
            const ratio = w / h;
            if (ratio > 1.7) return '16:9';
            if (ratio > 1.2) return '4:3';
            if (ratio > 0.9) return '1:1';
            if (ratio > 0.6) return '3:4';
            return '9:16';
        };
        const aspectRatio = getGeminiAspectRatio(imgWidth, imgHeight);
        console.log(`üìê Using aspect ratio: ${aspectRatio}`);

        const result = await model.generateContent({
            contents: [{ role: 'user', parts: contentParts }],
            generationConfig: {
                temperature: 0.6,
                // @ts-ignore - imageConfig is correct for image generation
                responseModalities: ['TEXT', 'IMAGE'],
            }
        });

        // Extract image from response
        let finalImageBase64: string | null = null;

        for (const candidate of result.response.candidates || []) {
            for (const part of candidate.content.parts || []) {
                if (part.inlineData?.mimeType?.startsWith('image')) {
                    finalImageBase64 = part.inlineData.data;
                    console.log('‚úÖ Found generated image');
                    break;
                }
            }
            if (finalImageBase64) break;
        }

        if (!finalImageBase64) {
            throw new Error('No image returned from Gemini');
        }

        // Step 4: Resize to match original dimensions
        console.log('üìê Resizing to original dimensions...');
        const resizedBuffer = await sharp(Buffer.from(finalImageBase64, 'base64'))
            .resize(imgWidth, imgHeight, {
                fit: 'fill',
                kernel: 'lanczos3'
            })
            .jpeg({ quality: 95 })
            .toBuffer();

        // Step 5: Upload to Firebase Storage
        let publicUrl: string;
        const imageId = `ai_edit_${Date.now()}`;

        if (bucket) {
            console.log('‚òÅÔ∏è  Uploading to Firebase...');
            const fileName = `projects/${projectId}/ai-edits/${imageId}.jpg`;
            const file = bucket.file(fileName);

            await file.save(resizedBuffer, {
                metadata: { contentType: 'image/jpeg' }
            });

            await file.makePublic();
            publicUrl = `https://storage.googleapis.com/${bucket.name}/${fileName}`;
        } else {
            publicUrl = `data:image/jpeg;base64,${resizedBuffer.toString('base64')}`;
            console.log('üìù Mock mode: returning base64');
        }

        // Step 6: Save to Firestore subcollection
        await db.collection('projects').doc(projectId).collection('images').doc(imageId).set({
            id: imageId,
            url: publicUrl,
            timestamp: FieldValue.serverTimestamp(),
            isOriginal: false,
            changes: [`AI Edit: ${prompt.slice(0, 100)}`],
            editType: 'custom-ai',
            width: imgWidth,
            height: imgHeight
        });

        // Step 7: Update user's AI credit usage (atomic transaction)
        await db.runTransaction(async (transaction) => {
            const freshUserDoc = await transaction.get(userRef);
            const freshData = freshUserDoc.data();
            const freshCustomAI = freshData?.customAI || {};

            const freshThisMonth = new Date().toISOString().slice(0, 7);
            const freshLastReset = (freshCustomAI.lastResetDate || '').slice(0, 7);

            if (freshThisMonth !== freshLastReset) {
                // Month changed, reset and set to 1
                transaction.update(userRef, {
                    'customAI.editsUsed': 1,
                    'customAI.lastResetDate': new Date().toISOString(),
                    'customAI.editsLimit': freshCustomAI.editsLimit || 5
                });
            } else {
                // Same month, increment
                transaction.update(userRef, {
                    'customAI.editsUsed': FieldValue.increment(1)
                });
            }
        });

        const renderTime = ((Date.now() - startTime) / 1000).toFixed(1);
        console.log(`‚úÖ AI Edit complete in ${renderTime}s`);

        res.json({
            success: true,
            renderedImageUrl: publicUrl,
            imageId,
            renderTime: parseFloat(renderTime),
            creditsRemaining: editsLimit - currentUsed - 1
        });

    } catch (error: any) {
        console.error('‚ùå AI Edit error:', error);

        res.status(500).json({
            success: false,
            error: error.message || 'AI edit failed',
            suggestion: getSuggestion(error)
        });
    }
});

// --- Error handler ---
app.use((err: Error, req: Request, res: Response, next: NextFunction) => {
    console.error('Unhandled error:', err);
    res.status(500).json({
        success: false,
        error: 'Internal server error',
        suggestion: 'Please try again later.'
    });
});

// --- Start server ---
const PORT = process.env.PORT || 3001;
app.listen(PORT, () => {
    console.log(`
üöÄ Render server running on http://localhost:${PORT}
üìç Endpoints:
   GET  /api/health
   POST /api/render-changes
   POST /api/render-placement
   POST /api/custom-ai-edit
    `);
});
